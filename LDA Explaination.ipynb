{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccd9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'learning' 'machine']\n",
      "[[0 1 1]\n",
      " [0 2 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Machine learning is amazing!\",\n",
    "             \"Deep learning is a subset of machine learning.\",\n",
    "             \"Natural language processing is a part of AI.\",\n",
    "             \"AI is transforming the world.\"]\n",
    "\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(documents)      # Convert text to document-term matrix\n",
    "print(cv.get_feature_names_out())      # Display remaining words\n",
    "print(dtm.toarray())                   # Show word counts in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524cb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n",
      "subset\n",
      "ai\n",
      "language\n",
      "deep\n",
      "key\n",
      "language\n",
      "subset\n",
      "subset\n",
      "deep\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\"Machine learning is amazing\",\n",
    "             \"Deep learning is a subset of machine learning\",\n",
    "             \"Natural Language Processing is a key part of AI\"]\n",
    "\n",
    "# Convert text into a document-term matrix\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "dtm = cv.fit_transform(documents)\n",
    "\n",
    "#Get feature names\n",
    "feature_names = cv.get_feature_names_out()   # Updated method.\n",
    "\n",
    "#Generate 10 random words from the vocabulary\n",
    "for i in range(10):\n",
    "    random_word_id = random.randint(0, len(feature_names) - 1)    # Use actual vocabulary size\n",
    "    print(feature_names[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c6dfdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is mostly about Topic 1\n",
      "Document 2 is mostly about Topic 0\n",
      "Document 3 is mostly about Topic 1\n",
      "Document 4 is mostly about Topic 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Sample corpus\n",
    "documents = [\"I love machine learning and data science\",\n",
    "             \"Deep learning is a subset of machine learning\",\n",
    "             \"I enjoy playing football and watching sports\",\n",
    "             \"Sports analytics is an interesting field\",\n",
    "]\n",
    "\n",
    "#Convert text to document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "dtm = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Train LDA model\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)        # n_components means document divide into two topic\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Get topic distribution for each document\n",
    "topic_results = lda.transform(dtm) # Shape: (num_docs, num_topics)\n",
    "\n",
    "# Identify the dominant topic for each document\n",
    "dominant_topics = topic_results.argmax(axis=1)\n",
    "\n",
    "# Print results\n",
    "for i, topic in enumerate(dominant_topics):\n",
    "    print(f\"Document {i+1} is mostly about Topic {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d58df1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff7511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
